{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b37c81f-eb71-4ffa-8e80-1e86e35c8f25",
   "metadata": {},
   "source": [
    "# Working with `gpytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdd206-1432-42bb-be0b-445084cee075",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7950ba-65b7-42a3-b4c7-6b72c55f4297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import malt\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from malt.molecule import Molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023d16a-0de3-4794-89e6-9786d0875d6d",
   "metadata": {},
   "source": [
    "Create fluorescence dataset. (for this notebook, skipping for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bef287-8b2c-4617-84fb-5b5f5c8f2ef4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def read_data():\n",
    "#     from pathlib import Path\n",
    "#     f = f'{Path.home()}/dev/choderalab/data/data/moonshot_fluorescence_titration_curves.csv'\n",
    "#     import pandas as pd\n",
    "#     df = pd.read_csv(f, index_col=0).dropna()\n",
    "#     df = df.rename({'concentration': 'c', 'inhibition': 'y'}, axis=1)\n",
    "#     return df\n",
    "\n",
    "# def parse_graph(smiles):\n",
    "#     from dgllife.utils import (\n",
    "#         smiles_to_bigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
    "#     )\n",
    "\n",
    "#     return smiles_to_bigraph(\n",
    "#         smiles = smiles,\n",
    "#         node_featurizer = CanonicalAtomFeaturizer(),\n",
    "#         edge_featurizer = CanonicalBondFeaturizer()\n",
    "#     )\n",
    "\n",
    "# def make_dataset():\n",
    "#     from malt import Dataset, AssayedMolecule\n",
    "#     from tqdm import tqdm\n",
    "    \n",
    "#     df = read_data()\n",
    "\n",
    "#     molecules = []\n",
    "#     for smiles, mol_metadata in tqdm(df.groupby('SMILES')):\n",
    "#         molecule = AssayedMolecule(\n",
    "#             smiles = smiles,\n",
    "#             # g = parse_graph(smiles),\n",
    "#             metadata = {'fluorescence': mol_metadata.drop('SMILES', axis=1).to_dict('records')}\n",
    "#         )\n",
    "#         molecules.append(molecule)\n",
    "\n",
    "#     # create dataset\n",
    "#     data = Dataset(molecules)\n",
    "#     return data\n",
    "\n",
    "# data = make_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149362f5-ea35-4bf3-8e13-3a35942c0c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/1128\n"
     ]
    }
   ],
   "source": [
    "from malt.data.collections import esol\n",
    "\n",
    "data = esol()\n",
    "g, y = data.batch(by=['g', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d2c7c-09a1-456c-9c94-a69781a10e6c",
   "metadata": {},
   "source": [
    "Make model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e2d9d7-df9f-410f-9fc4-df4101c9ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.LinearMean(128)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "class GPyTorchSupervisedModel(gpytorch.models.GP):\n",
    "    def __init__(self, representation, regressor, likelihood, train_x, train_y):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                - gp (gpytorch.models.ExactGP): A GP that expects to operate on features extracted by a GCN.\n",
    "                - gcn (torch.nn.Module): Some PyTorch module that extracts graph features.\n",
    "                - train_x (any input to gcn): The training data as expected by the GCN\n",
    "                - train_y (torch.Tensor): Training labels\n",
    "        \"\"\"\n",
    "        super(GPyTorchSupervisedModel, self).__init__()\n",
    "        self.representation = representation\n",
    "        self.regressor = regressor\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "    \n",
    "    def forward(self, g):\n",
    "        train_h = self.representation(self.train_x)\n",
    "        self.regressor.set_train_data(train_h, self.train_y)\n",
    "\n",
    "        h = self.representation(g)\n",
    "        # self.regressor.set_train_data(h, self.train_y, strict=False)\n",
    "        return self.regressor(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95aca66-1e26-4ba7-8919-4fa2d5fb10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import malt\n",
    "\n",
    "device = 'cuda:0'\n",
    "x_tr, y_tr = g.to(device), y.to(device)\n",
    "\n",
    "# initialize\n",
    "representation = malt.models.representation.DGLRepresentation(\n",
    "        out_features=128,\n",
    ").cuda()\n",
    "\n",
    "# instantiate the GPyTorch model\n",
    "h = representation(x_tr)\n",
    "\n",
    "# initialize likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "\n",
    "# instantiate regressor\n",
    "regressor = ExactGPModel(h, y_tr, likelihood).cuda()\n",
    "\n",
    "# initialize model\n",
    "model = GPyTorchSupervisedModel(\n",
    "    representation, regressor, likelihood,\n",
    "    train_x=x_tr, train_y=y_tr\n",
    ")\n",
    "\n",
    "# sns.displot(model(g.to('cuda:0')).loc.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035f805-b725-4b45-a91b-5b8adb613460",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999eb82d-8670-43be-a005-ba36fe46c898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_iter = 1000\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "likelihood.train()\n",
    "model.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Includes GaussianLikelihood parameters\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Calc loss and backprop gradients\n",
    "    y_target = model(x_tr)\n",
    "    loss = -mll(y_target, y_tr).sum()\n",
    "    loss.backward()\n",
    "    if i == 1:\n",
    "        break\n",
    "    # print('Iter %d/%d - Loss: %.3f   rep_param: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #     i + 1, training_iter, loss.item(),\n",
    "    #     next(model.representation.parameters())[0, 0].item(),\n",
    "    #     model.regressor.covar_module.base_kernel.lengthscale.item(),\n",
    "    #     model.regressor.likelihood.noise.item()\n",
    "    # ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cafc1ca-423f-4b07-b996-0ddbad7e1734",
   "metadata": {},
   "source": [
    "Debug gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5297710-46d7-4035-b165-ba7cf8ff00cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'representation.embedding_in.0.weight': 11.14016342163086,\n",
       " 'representation.embedding_in.0.bias': 151.15235900878906,\n",
       " 'representation.gn0.weight': 16.048030853271484,\n",
       " 'representation.gn0.bias': 288.2734375,\n",
       " 'representation.gn1.weight': 15.439170837402344,\n",
       " 'representation.gn1.bias': 558.8292236328125,\n",
       " 'representation.gn2.weight': 15.742813110351562,\n",
       " 'representation.gn2.bias': 1154.57763671875,\n",
       " 'representation.embedding_out.0.weight': 27.92206382751465,\n",
       " 'representation.embedding_out.0.bias': 3839.130859375,\n",
       " 'representation.ff.0.weight': 288.83917236328125,\n",
       " 'representation.ff.0.bias': 181.6907958984375,\n",
       " 'regressor.likelihood.noise_covar.raw_noise': 166.24801635742188,\n",
       " 'regressor.mean_module.weights': 192.01132202148438,\n",
       " 'regressor.mean_module.bias': 226.61843872070312,\n",
       " 'regressor.covar_module.raw_outputscale': 1673.2318115234375,\n",
       " 'regressor.covar_module.base_kernel.raw_lengthscale': 1317.110595703125}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{n: p.grad.abs().mean().item() for n, p in model.named_parameters()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

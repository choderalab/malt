{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cde149-8b0f-4a9f-82c3-cf1286887a65",
   "metadata": {},
   "source": [
    "# Exact GP playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d98508-3546-4ab6-98d1-b4404b748d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import malt\n",
    "import abc\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce02c6-af15-4290-ba79-a340a4617ee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pre-reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34b586f-044a-4392-adfd-ce60af117545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KERNELS\n",
    "# =============================================================================\n",
    "class RBF(torch.nn.Module):\n",
    "    r\"\"\"A Gaussian Process Kernel that hosts parameters.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    l could be either of shape 1 or hidden dim\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, scale=0.0, variance=0.0, ard=True):\n",
    "\n",
    "        super(RBF, self).__init__()\n",
    "\n",
    "        if ard is True:\n",
    "            self.scale = torch.nn.Parameter(scale * torch.ones(in_features))\n",
    "\n",
    "        else:\n",
    "            self.scale = torch.nn.Parameter(torch.tensor(scale))\n",
    "\n",
    "        self.variance = torch.nn.Parameter(torch.tensor(variance))\n",
    "\n",
    "    def distance(self, x, x_):\n",
    "        \"\"\" Distance between data points. \"\"\"\n",
    "        return torch.norm(x[:, None, :] - x_[None, :, :], p=2, dim=2)\n",
    "\n",
    "    def forward(self, x, x_=None):\n",
    "        \"\"\" Forward pass. \"\"\"\n",
    "        # replicate x if there's no x_\n",
    "        if x_ is None:\n",
    "            x_ = x\n",
    "\n",
    "        # for now, only allow two dimension\n",
    "        assert x.dim() == 2\n",
    "        assert x_.dim() == 2\n",
    "\n",
    "        x = x * torch.exp(self.scale)\n",
    "        x_ = x_ * torch.exp(self.scale)\n",
    "\n",
    "        # (batch_size, batch_size)\n",
    "        distance = self.distance(x, x_)\n",
    "\n",
    "        # convariant matrix\n",
    "        # (batch_size, batch_size)\n",
    "        k = torch.exp(self.variance) * torch.exp(-0.5 * distance)\n",
    "\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8902955-dd8e-46b5-9c17-1cba6f713da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(torch.nn.Module, abc.ABC):\n",
    "    \"\"\"Base class for a regressor.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, *args, **kwargs):\n",
    "        super(Regressor, self).__init__(*args, **kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "class ExactGaussianProcessRegressor(Regressor, gpytorch.models.GP):\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int = 128,\n",
    "        out_features: int = 2,\n",
    "        kernel_factory: torch.nn.Module = RBF,\n",
    "        log_sigma: float = -3.0,\n",
    "    ):\n",
    "        assert out_features == 2\n",
    "        super(ExactGaussianProcessRegressor, self).__init__(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "        )\n",
    "\n",
    "        # construct kernel\n",
    "        self.kernel = kernel_factory(\n",
    "            in_features=in_features,\n",
    "        )\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.log_sigma = torch.nn.Parameter(\n",
    "            torch.tensor(log_sigma),\n",
    "        )\n",
    "\n",
    "    def _get_kernel_and_auxiliary_variables(\n",
    "        self,\n",
    "        x_tr,\n",
    "        y_tr,\n",
    "        x_te=None,\n",
    "    ):\n",
    "        \"\"\" Get kernel and auxiliary variables for forward pass. \"\"\"\n",
    "\n",
    "        # compute the kernels\n",
    "        k_tr_tr = self._perturb(self.kernel.forward(x_tr, x_tr))\n",
    "\n",
    "        if x_te is not None:  # during test\n",
    "            k_te_te = self._perturb(self.kernel.forward(x_te, x_te))\n",
    "            k_te_tr = self._perturb(self.kernel.forward(x_te, x_tr))\n",
    "            # k_tr_te = self.forward(x_tr, x_te)\n",
    "            k_tr_te = k_te_tr.t()  # save time\n",
    "\n",
    "        else:  # during train\n",
    "            k_te_te = k_te_tr = k_tr_te = k_tr_tr\n",
    "\n",
    "        # (batch_size_tr, batch_size_tr)\n",
    "        k_plus_sigma = k_tr_tr + torch.exp(self.log_sigma) * torch.eye(\n",
    "            k_tr_tr.shape[0], device=k_tr_tr.device\n",
    "        )\n",
    "\n",
    "        # (batch_size_tr, batch_size_tr)\n",
    "        l_low = torch.cholesky(k_plus_sigma)\n",
    "        l_up = l_low.t()\n",
    "\n",
    "        # (batch_size_tr. 1)\n",
    "        l_low_over_y, _ = torch.triangular_solve(\n",
    "            input=y_tr, A=l_low, upper=False\n",
    "        )\n",
    "\n",
    "        # (batch_size_tr, 1)\n",
    "        alpha, _ = torch.triangular_solve(\n",
    "            input=l_low_over_y, A=l_up, upper=True\n",
    "        )\n",
    "\n",
    "        return k_tr_tr, k_te_te, k_te_tr, k_tr_te, l_low, alpha\n",
    "\n",
    "    def condition(self, x_te, *args, x_tr=None, y_tr=None, **kwargs):\n",
    "        r\"\"\"Calculate the predictive distribution given `x_te`.\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        Here we allow the speicifaction of sampler but won't actually\n",
    "        use it here in this version.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_te : `torch.Tensor`, `shape=(n_te, hidden_dimension)`\n",
    "            Test input.\n",
    "\n",
    "        x_tr : `torch.Tensor`, `shape=(n_tr, hidden_dimension)`\n",
    "             (Default value = None)\n",
    "             Training input.\n",
    "\n",
    "        y_tr : `torch.Tensor`, `shape=(n_tr, 1)`\n",
    "             (Default value = None)\n",
    "             Test input.\n",
    "\n",
    "        sampler : `torch.optim.Optimizer` or `pinot.Sampler`\n",
    "             (Default value = None)\n",
    "             Sampler.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        distribution : `torch.distributions.Distribution`\n",
    "            Predictive distribution.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # get parameters\n",
    "        (\n",
    "            k_tr_tr,\n",
    "            k_te_te,\n",
    "            k_te_tr,\n",
    "            k_tr_te,\n",
    "            l_low,\n",
    "            alpha,\n",
    "        ) = self._get_kernel_and_auxiliary_variables(x_tr, y_tr, x_te)\n",
    "\n",
    "        # compute mean\n",
    "        # (batch_size_te, 1)\n",
    "        mean = k_te_tr @ alpha\n",
    "\n",
    "        # (batch_size_tr, batch_size_te)\n",
    "        v, _ = torch.triangular_solve(input=k_tr_te, A=l_low, upper=False)\n",
    "\n",
    "        # (batch_size_te, batch_size_te)\n",
    "        variance = k_te_te - v.t() @ v\n",
    "\n",
    "        # ensure symetric\n",
    "        variance = 0.5 * (variance + variance.t())\n",
    "\n",
    "        # $ p(y|X) = \\int p(y|f)p(f|x) df $\n",
    "        # variance += torch.exp(self.log_sigma) * torch.eye(\n",
    "        #         *variance.shape,\n",
    "        #         device=variance.device)\n",
    "\n",
    "        # construct noise predictive distribution\n",
    "        distribution = (\n",
    "            torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "                mean.flatten(), variance\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return distribution\n",
    "\n",
    "    def _perturb(self, k):\n",
    "        \"\"\"Add small noise `epsilon` to the diagonal of covariant matrix.\n",
    "        Parameters\n",
    "        ----------\n",
    "        k : `torch.Tensor`, `shape=(n_data_points, n_data_points)`\n",
    "            Covariant matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        k : `torch.Tensor`, `shape=(n_data_points, n_data_points)`\n",
    "            Preturbed covariant matrix.\n",
    "        \"\"\"\n",
    "        # introduce noise along the diagonal\n",
    "        noise = self.epsilon * torch.eye(*k.shape, device=k.device)\n",
    "\n",
    "        return k + noise\n",
    "\n",
    "    def loss(self, x_tr, y_tr, *args, **kwargs):\n",
    "        r\"\"\"Compute the loss.\n",
    "        Note\n",
    "        ----\n",
    "        Defined to be negative Gaussian likelihood.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_tr : `torch.Tensor`, `shape=(n_training_data, hidden_dimension)`\n",
    "            Input of training data.\n",
    "        y_tr : `torch.Tensor`, `shape=(n_training_data, 1)`\n",
    "            Target of training data.\n",
    "        Returns\n",
    "        -------\n",
    "        nll : `torch.Tensor`, `shape=(,)`\n",
    "            Negative log likelihood.\n",
    "        \"\"\"\n",
    "#         # point data to object\n",
    "#         self._x_tr = x_tr\n",
    "#         self._y_tr = y_tr\n",
    "\n",
    "#         # get the parameters\n",
    "#         (\n",
    "#             k_tr_tr,\n",
    "#             k_te_te,\n",
    "#             k_te_tr,\n",
    "#             k_tr_te,\n",
    "#             l_low,\n",
    "#             alpha,\n",
    "#         ) = self._get_kernel_and_auxiliary_variables(x_tr, y_tr)\n",
    "\n",
    "#         import math\n",
    "\n",
    "#         # we return the exact nll with constant\n",
    "#         nll = (\n",
    "#             0.5 * (y_tr.t() @ alpha)\n",
    "#             + torch.trace(l_low)\n",
    "#             + 0.5 * y_tr.shape[0] * math.log(2.0 * math.pi)\n",
    "#         )\n",
    "        \n",
    "        preds = self.condition(x_tr, x_tr=x_tr, y_tr=y_tr)\n",
    "        # nll = preds.log_prob(y_tr.ravel())\n",
    "        \n",
    "        import gpytorch\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, self)\n",
    "\n",
    "        output = gpytorch.distributions.MultivariateNormal(preds.loc.cpu(), preds.covariance_matrix.cpu())\n",
    "        nll = -mll(output, y_tr.cpu()).mean()\n",
    "        \n",
    "        return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538018d-4d6f-480f-8173-aaa5235a93d3",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62cb1568-992e-490a-b33c-1e3548e491bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f9cb137e30>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ee9c2a-730a-495d-a93c-0d0c3839cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mvn = net.condition(graph)\n",
    "\n",
    "# torch.exp(mvn.log_prob(torch.tensor([[5.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c50ee7f-ccaa-4496-a492-99f2cf718994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import dgl\n",
    "# import malt\n",
    "\n",
    "# y = torch.Tensor([[5.0]])\n",
    "# mol = malt.Molecule(\"C\")\n",
    "# mol.featurize()\n",
    "# graph = dgl.batch([mol.g])\n",
    "\n",
    "# y = net.loss(graph, torch.tensor([[5.0]]))\n",
    "# # y.backward()\n",
    "\n",
    "# # y = net.condition(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6b4439b-7407-4bc5-84fb-8391f9366a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malt.data.collections import linear_alkanes\n",
    "\n",
    "la_data = linear_alkanes()\n",
    "g, y = la_data.batch()\n",
    "\n",
    "net = malt.models.supervised_model.GaussianProcessSupervisedModel(\n",
    "    representation=malt.models.representation.DGLRepresentation(\n",
    "        out_features=128\n",
    "    ),\n",
    "    regressor=ExactGaussianProcessRegressor(\n",
    "        in_features=128,\n",
    "        out_features=2,\n",
    "    ),\n",
    "    likelihood=malt.models.likelihood.HeteroschedasticGaussianLikelihood(),\n",
    ").cuda()\n",
    "\n",
    "l = net.loss(g, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20c19b94-5c46-4a72-a636-74d553a43a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2379, device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f2bb56-eb91-47fa-8bfe-504cc39de160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpytorch\n",
    "# likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "# mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, net)\n",
    "\n",
    "# output = net.condition(g)\n",
    "# output = gpytorch.distributions.MultivariateNormal(output.loc.cpu(), output.covariance_matrix.cpu())\n",
    "# print(-mll(output, y.cpu()).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1860d-1373-4efc-a84e-32780e6408db",
   "metadata": {},
   "source": [
    "# Test with ESOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52182265-7391-487a-a476-83c49c13bfcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/1128\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "import malt\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data\", type=str, default=\"esol\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"gp\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "data = getattr(malt.data.collections, args.data)()\n",
    "\n",
    "def test(data):\n",
    "    \n",
    "    data.shuffle(seed=1)\n",
    "    ds_tr_vl, ds_te = data.split([0.9, 0.1])\n",
    "    ds_tr, _ = ds_tr_vl.split([0.9, 0.1])\n",
    "    \n",
    "    if args.model == \"gp\":\n",
    "        model = malt.models.supervised_model.GaussianProcessSupervisedModel(\n",
    "            representation=malt.models.representation.DGLRepresentation(\n",
    "                out_features=32, hidden_features=32,\n",
    "            ),\n",
    "            regressor=malt.models.regressor.ExactGaussianProcessRegressor(\n",
    "                train_targets=ds_tr.batch(by='y'), in_features=32, out_features=2,\n",
    "            ),\n",
    "            likelihood=malt.models.likelihood.HeteroschedasticGaussianLikelihood(),\n",
    "        ).cuda()\n",
    "\n",
    "\n",
    "    elif args.model == \"nn\":\n",
    "        model = malt.models.supervised_model.SimpleSupervisedModel(\n",
    "            representation=malt.models.representation.DGLRepresentation(\n",
    "                out_features=32,\n",
    "            ),\n",
    "            regressor=malt.models.regressor.NeuralNetworkRegressor(\n",
    "                in_features=32, out_features=1,\n",
    "            ),\n",
    "            likelihood=malt.models.likelihood.HomoschedasticGaussianLikelihood(),\n",
    "        ).cuda()\n",
    "\n",
    "\n",
    "    trainer = malt.trainer.get_default_trainer(\n",
    "        without_player=True,\n",
    "        n_epochs=100,\n",
    "        learning_rate=1e-3,\n",
    "    )\n",
    "    \n",
    "    mll = malt.models.marginal_likelihood.ExactMarginalLogLikelihood(\n",
    "        model.likelihood,\n",
    "        model\n",
    "    )\n",
    "    \n",
    "    # return model\n",
    "    model = trainer(model, ds_tr_vl, mll)\n",
    "\n",
    "    # model_cpu = model.to('cpu')\n",
    "    model.eval()\n",
    "    r2 = malt.metrics.supervised_metrics.R2()(model, ds_tr)\n",
    "\n",
    "    # rmse = malt.metrics.supervised_metrics.RMSE()(model, ds_te)\n",
    "    # print(rmse)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be79d71b-f1c9-4258-829d-c6b2aeed178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c49113a1-ff0a-429f-8823-e403619cb7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9924, device='cuda:0', grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2950cc88-67be-4b1d-b0bf-6e7eb7863e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shuffle(seed=1)\n",
    "# ds_tr, ds_te = data.split([8, 2])\n",
    "# train_targets = ds_tr.batch(by='y')\n",
    "\n",
    "# reg = malt.models.regressor.ExactGaussianProcessRegressor(\n",
    "#     train_targets=train_targets,\n",
    "#     in_features=32,\n",
    "#     out_features=2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3eec678-ee82-411c-990f-8885f5838881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_tr, ds_te = data.split([9, 1])\n",
    "# g, y = ds_tr.batch()\n",
    "# model.representation(g).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ecef43f-d69f-4ee3-a77c-8e5e7e64875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tr_vl, ds_te = data.split([0.9, 0.1])\n",
    "ds_tr, _ = ds_tr_vl.split([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0be96e81-9860-4ee6-9b85-e7b746e2499a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset with 913 molecules"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e59a3c03-09e7-4175-998d-b4f4fee1533c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_tr_vl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mds_tr_vl\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds_tr_vl' is not defined"
     ]
    }
   ],
   "source": [
    "ds_tr_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe10a3a-8b17-46a2-ac5d-0ca001055f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([913, 1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m mll \u001b[38;5;241m=\u001b[39m malt\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mmarginal_likelihood\u001b[38;5;241m.\u001b[39mExactMarginalLogLikelihood(\n\u001b[0;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39mlikelihood,\n\u001b[0;32m     40\u001b[0m     model\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# return model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_tr_vl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# model_cpu = model.to('cpu')\u001b[39;00m\n\u001b[0;32m     47\u001b[0m r2 \u001b[38;5;241m=\u001b[39m malt\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39msupervised_metrics\u001b[38;5;241m.\u001b[39mR2()(model, ds_te)\n",
      "File \u001b[1;32mc:\\users\\micha\\dev\\choderalab\\malt\\malt\\trainer.py:111\u001b[0m, in \u001b[0;36mget_default_trainer.<locals>._default_trainer_without_player\u001b[1;34m(model, ds, marginal_likelihood, optimizer, learning_rate, n_epochs, batch_size, min_learning_rate, warmup)\u001b[0m\n\u001b[0;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    110\u001b[0m output \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m--> 111\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmarginal_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    112\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    113\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\malt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\micha\\dev\\choderalab\\malt\\malt\\models\\marginal_likelihood.py:66\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, output, target, *args, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, output, target, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     61\u001b[0m     alpha, l_low \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mregressor\u001b[38;5;241m.\u001b[39mmll_vars\n\u001b[0;32m     63\u001b[0m     mll \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (target\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m@\u001b[39m alpha)\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtrace(l_low)\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m target\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)\n\u001b[0;32m     67\u001b[0m     )\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mll\n",
      "\u001b[1;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "data.shuffle(seed=1)\n",
    "ds_tr_vl, ds_te = data.split([0.9, 0.1])\n",
    "ds_tr, _ = ds_tr_vl.split([0.9, 0.1])\n",
    "\n",
    "print(ds_tr.batch(by='y').shape)\n",
    "\n",
    "if args.model == \"gp\":\n",
    "    model = malt.models.supervised_model.GaussianProcessSupervisedModel(\n",
    "        representation=malt.models.representation.DGLRepresentation(\n",
    "            out_features=32, hidden_features=32,\n",
    "        ),\n",
    "        regressor=malt.models.regressor.ExactGaussianProcessRegressor(\n",
    "            train_targets=ds_tr.batch(by='y'), in_features=32, out_features=2,\n",
    "        ),\n",
    "        likelihood=malt.models.likelihood.HeteroschedasticGaussianLikelihood(),\n",
    "    ).cuda()\n",
    "\n",
    "\n",
    "elif args.model == \"nn\":\n",
    "    model = malt.models.supervised_model.SimpleSupervisedModel(\n",
    "        representation=malt.models.representation.DGLRepresentation(\n",
    "            out_features=32,\n",
    "        ),\n",
    "        regressor=malt.models.regressor.NeuralNetworkRegressor(\n",
    "            in_features=32, out_features=1,\n",
    "        ),\n",
    "        likelihood=malt.models.likelihood.HomoschedasticGaussianLikelihood(),\n",
    "    ).cuda()\n",
    "\n",
    "\n",
    "trainer = malt.trainer.get_default_trainer(\n",
    "    without_player=True,\n",
    "    batch_size=len(ds_tr_vl),\n",
    "    n_epochs=100,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "mll = malt.models.marginal_likelihood.ExactMarginalLogLikelihood(\n",
    "    model.likelihood,\n",
    "    model\n",
    ")\n",
    "\n",
    "# return model\n",
    "model = trainer(model, ds_tr_vl, mll)\n",
    "\n",
    "# model_cpu = model.to('cpu')\n",
    "r2 = malt.metrics.supervised_metrics.R2()(model, ds_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
